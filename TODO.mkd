- [X] Single model for all nodes
- [X] All nodes in the same directory
- [X] Compile repo, computing the schema of source nodes
- [X] API for running sync queries
- [X] API for running async queries
- [X] Results backend
- [X] Celery workers for async queries
- [X] Add DJ and Redis to docker-compose
- [X] Queries with multiple statements
- [X] Return statement SQL with `results`
- [X] Add Celery to docker-compose
- [X] Representations should have columns
- [X] Paginating results
- [X] Compute the schema of downstream nodes
- [X] Organize files (`api/`, `models/`, etc)
- [X] Make database/node name unique
- [X] Split models tests
- [X] Rename `queries.py` to `engine.py`
- [X] Name/description for the server in `.env`
- [X] Add an enum for types, instead of using strings
- [X] Add custom type for the parse tree
- [X] Translate metrics into SQL
- [X] Metrics API
- [X] Allow only 1 aggregation (metric)
- [X] Allow grouping in metrics API
- [X] Allow filtering in metrics API
- [ ] Allow metrics to be queried via SQL
- [ ] Handle columns with database-specific names (`__timestamp` in Druid, eg)
- [ ] Write columns back to YAML -> way to add column metadata (dimension and more)
- [ ] Limit results
- [ ] Optimize data transfer (delta-of-delta for timeseries, msgpack?)
- [ ] Compute statistics on columns (histogram, correlation)
- [ ] Move data on JOINs based on column statistics
- [ ] Virtual dimensions (time, space, user-defined)
- [ ] JS dataframe with time-aware caching and additive-aware, to reuse queries
- [ ] 2 modes of join: Shillelagh and move data
- [ ] Auto-map dimensions from the DB schema?
- [ ] UUID for models?

Integration with Superset:

1. Run `superset sync dj http://dj.example.com/`
2. Creates database, a `metrics` datasets, and add metrics as custom SQL (metric `foo` with SQL `foo`)
3. Depending on the metric selected in Explore bring in dimensions (AirBnB has that?)

Node types:

- source
- transform
- dimension
- metric
- population

Relationships:

- source -> transform [-> transform ]-> metric
- source -> dimension -> population
- population based on metrics as well?

DSL for querying:

- `m=likes,comments&d=user.country&f=userid>10,time>2021-01-01T00:00:00+00:00`

Examples:

1. Dimension table in 2 storages (fast/slow), choose fast.
2. Dimension table in 2 storages (fast/slow) but only a few columns in fast; choose slow.
3. Translate query with cross-DB join, `FROM PROGRAM` in Postgres
