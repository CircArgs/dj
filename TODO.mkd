- [X] Single model for all nodes
- [X] All nodes in the same directory
- [X] Compile repo, computing the schema of source nodes
- [X] API for running sync queries
- [X] API for running async queries
- [X] Results backend
- [X] Celery workers for async queries
- [X] Add DJ and Redis to docker-compose
- [X] Queries with multiple statements
- [X] Return statement SQL with `results`
- [X] Add Celery to docker-compose
- [X] Representations should have columns
- [X] Paginating results
- [X] Compute the schema of downstream nodes
- [X] Organize files (`api/`, `models/`, etc)
- [ ] Translate metrics into SQL
- [ ] Rename `queries.py` to `engine.py`
- [ ] Split models tests
- [ ] Make database/node name unique
- [ ] Compute metrics (ie, run query)
- [ ] Allow only 1 aggregation (metric)
- [ ] Add an enum for types, instead of using strings (also, a type for a parse tree)
- [ ] Limit results
- [ ] Write columns back to YAML -> way to add column metadata (dimension and more)
- [ ] Optimize data transfer (delta-of-delta for timeseries, msgpack?)
- [ ] Compute statistics on columns (histogram, correlation)
- [ ] Move data on JOINs based on column statistics
- [ ] Virtual dimensions (time, space, user-defined)
- [ ] JS dataframe with time-aware caching and additive-aware, to reuse queries
- [ ] 2 modes of join: Shillelagh and move data
- [ ] Auto-map dimensions from the DB schema?
- [ ] UUID for models?
- [ ] Name/description for the server in `.env`

Integration with Superset:

1. Run `superset sync dj http://dj.example.com/`
2. Creates database, a `metrics` datasets, and add metrics as custom SQL (metric `foo` with SQL `foo`)
3. Depending on the metric selected in Explore bring in dimensions (AirBnB has that?)

Node types:

- source
- transform
- dimension
- metric
- population

Relationships:

- source -> transform [-> transform ]-> metric
- source -> dimension -> population
- population based on metrics as well?

DSL for querying:

- `m=likes,comments&d=user.country&f=userid>10,time>2021-01-01T00:00:00+00:00`

Examples:

1. Dimension table in 2 storages (fast/slow), choose fast.
2. Dimension table in 2 storages (fast/slow) but only a few columns in fast; choose slow.
3. Translate query with cross-DB join, `FROM PROGRAM` in Postgres

SQL input:

```
SELECT core.likes FROM metrics
WHERE user.country = 'BR'
```

Gets translated to:

```
SELECT COUNT(*) FROM content_actions
JOIN dim_users ON content_actions.user_id = dim_users.id
```
